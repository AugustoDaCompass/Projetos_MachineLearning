# -*- coding: utf-8 -*-
"""UDEMY.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JPul2GbcZFppI0jnRDlRckofPT8BXJpM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import re
import nltk

import warnings
warnings.filterwarnings('ignore')

pd.set_option('display.max_rows',200)
pd.set_option('display.max_columns',100)

plt.rcParams['figure.figsize'] = (12,6)
plt.style.use('seaborn-darkgrid')

base_dados = pd.read_csv('arquivo.csv')



base_dados.head()

#modelagem

nulos = base_dados.isnull()
sns.heatmap(nulos,cbar=False)

colunas_extrair = base_dados.columns[10:]
base_dados.drop(columns=colunas_extrair,inplace=True)

base_dados.shape

base_dados.nunique()

base_dados.drop(columns='Unnamed: 0',inplace=True)

base_dados.info()

from dateutil.parser import parse

teste = base_dados['Created At'][0]
teste
print(f'{parse(teste).date()}')

def ajustar_data(data):
  teste =  data
  transformar = parse(teste)
  data_final = transformar.date()
  return data_final


base_dados['data_formatada'] = base_dados['Created At'].apply(ajustar_data)
base_dados.iloc[0:5, 5:]



base_dados['data_formatada'] = pd.to_datetime(base_dados['data_formatada'])

base_dados['ano'] = base_dados['data_formatada'].dt.year
base_dados['dia'] = base_dados['data_formatada'].dt.day
base_dados['mes'] = base_dados['data_formatada'].dt.month

base_dados.groupby(by=['ano','mes']).agg(
    quantidade = ('Username','count')
)

#ajustando localizacao
base_dados.isnull().sum()

analise_localiacao =  base_dados.loc[
    (base_dados['Geo Coordinates.latitude'].notnull()) &
    (base_dados['Geo Coordinates.longitude'].notnull())
]

analise_localiacao.shape

localizacao = analise_localiacao.groupby(by=['Geo Coordinates.latitude','Geo Coordinates.longitude']).count()[['Username']].reset_index()

localizacao.columns = ['lat','lon','quantidade']
localizacao.head()

px.density_mapbox(
    localizacao,
    lat='lat',
    lon='lon',
    z='quantidade',
    mapbox_style = 'stamen-terrain',
    center = dict(lat=-23.700,lon=-46.555),
    zoom=2,
    radius=30
)

base_dados['User Location'].head(30)

#!pip install geopy

from geopy.geocoders import Nominatim

experimento_funcao = Nominatim(user_agent='GetLoc')

experimento_funcao

analise_local = base_dados['User Location'].value_counts(normalize=True).cumsum().reset_index()

analise_local = analise_local.loc[analise_local['User Location'] < 0.5]

analise_local.columns = ['local','percentual']

analise_local.head()

quantidade = base_dados['User Location'].value_counts().reset_index()

quantidade.columns = ['local','quantidade']

tab_localizacao = pd.merge(analise_local, quantidade,on=['local'],how='inner')

tab_localizacao.head()

tab_localizacao['longitude'] = None
tab_localizacao['latitude'] = None


for i in range(0,len(tab_localizacao['local'])):

  try:
    local = experimento_funcao.geocode(tab_localizacao['local'][i])
  except:
    pass

  if local!= None:
    tab_localizacao['latitude'][i] = local.latitude
    tab_localizacao['longitude'][i] = local.longitude




  print(i)

tab_localizacao.head()

px.density_mapbox(
    tab_localizacao,
    lat='latitude',
    lon='longitude',
    z='quantidade',
    mapbox_style = 'stamen-terrain',
    center = dict(lat=-23.700,lon=-46.555),
    zoom=2,
    radius=30
)

base_dados['Username'].value_counts()

from yellowbrick import ClassBalance
grafico_balanco = ClassBalance(labels=['Neutro','Negativo','Positivo'])
grafico_balanco.fit(base_dados['Classificacao'])
grafico_balanco.show()

analise_valores = base_dados['Classificacao'].value_counts()

grafico_balanco2 = go.Figure(
    go.Funnelarea(
        text=analise_valores.index,
        values=analise_valores.values,
        title={'text':'distribuicao de sentimentos','position':'top center'}

    )
)
grafico_balanco2.show()

def calcular_tamanho(palavras):
  quebrado = palavras.split()
  quantidade = len(quebrado)
  return quantidade




base_dados['quantidade_palavras'] = base_dados['Text'].apply(calcular_tamanho)

px.box(base_dados,x='Classificacao',y='quantidade_palavras',title='quantidade de palavras por sentimento')





concatenar = ''
for i in base_dados['Text']:
  for j in i.split():
    concatenar =  concatenar + ' ' + j

from wordcloud import WordCloud

nuvem_palavras = WordCloud(width=1600,height=1000).generate(concatenar)

figura,eixo = plt.subplots(figsize=(15,7))

eixo.imshow(nuvem_palavras,interpolation='bilinear')
eixo.set_axis_off()

palavras_positivas = ''
palavras_negativas = ''
palavras_neutras = ''

for sentimentos in base_dados['Classificacao'].unique():
  if sentimentos == 'Positivo':
    filtro = base_dados.loc[base_dados['Classificacao'] == sentimentos]

    for palavra in filtro['Text']:
      for i in palavra.split():
        palavras_positivas = palavras_positivas + ' ' + i

  elif sentimentos == 'Negativo':
    filtro = base_dados.loc[base_dados['Classificacao'] == sentimentos]

    for palavra in filtro['Text']:
      for i in palavra.split():
        palavras_negativas = palavras_negativas + ' ' + i
  else:
    filtro = base_dados.loc[base_dados['Classificacao'] == sentimentos]

    for palavra in filtro['Text']:
      for i in palavra.split():
        palavras_neutras = palavras_neutras + ' ' + i






print(len(palavras_positivas))

nuvem_palavras = WordCloud(width=1600,height=1000,background_color='green').generate(palavras_positivas)

figura,eixo = plt.subplots(figsize=(15,7))

eixo.imshow(nuvem_palavras,interpolation='bilinear')
eixo.set_axis_off()

from collections import Counter

quebra_frases  = base_dados['Text'].apply(lambda x:str(x).split())

dados_rank = Counter([item for sublista in quebra_frases for item in sublista])

tab_quantidade = pd.DataFrame(dados_rank.most_common(20))

tab_quantidade.columns = ['palavra','qtd']

figura = px.bar(tab_quantidade,x='qtd',y='palavra',color='palavra',orientation='h',width=700,height=700)
figura.show()



"""***MINERACAO*** ***DE*** ***TEXTO***"""

copia = base_dados.copy()

base_dados.drop_duplicates(['Text'],inplace=True)

nltk.download('stopwords')

nltk.corpus.stopwords.words('portuguese')[0:20]

nltk.download('rstp')
nltk.download('rslp')

radical = nltk.stem.RSLPStemmer()

palavras = ['gostei','gostam','gostaram','gostamos','gostarÃ£o']

for i in palavras:
  print(i,'=' ,radical.stem(i))





"""# **EXPRESSOES** **REGULARES**

"""

frases = 'acessei o site do google https://www.google.com.br/?hl=pt-BR e achei legal!!'

re.findall('google',frases)

re.findall('[a-d]',frases)
re.findall('ac..',frases)





"""**FREQUENCIA DAS PALAVRAS
**
"""

palavras_teste = 'hoje o dia esta lindo, e neste dia vou fazer um lindo video de dados'

freq = nltk.FreqDist(palavras_teste.split())

freq.most_common()





"""**LIMPAR** ***OS*** **DADOS**"""

exemplo =  str(base_dados['Text'])


def limpar_dados(texto):

  camada1 = re.sub('https:\S+','',texto)

  camada2 = camada1.lower()

  camada3 = re.sub('["("")"?@|$|.|!,:%;"]','',camada2)

  Retirar_Emojis = re.compile(
  "["
      u"\U0001F600-\U0001F64F"  # Emojis
      u"\U0001F300-\U0001F5FF"  # Simbolos
      u"\U0001F680-\U0001F6FF"
      u"\U0001F1E0-\U0001F1FF"
      u"\U0001F1F2-\U0001F1F4"
      u"\U0001F1E6-\U0001F1FF"
      u"\U0001F600-\U0001F64F"
      u"\U00002702-\U000027B0"
      u"\U000024C2-\U0001F251"
      u"\U0001f926-\U0001f937"
      u"\U0001F1F2"
      u"\U0001F1F4"
      u"\U0001F620"
      u"\u200d"
      u"\u2640-\u2642"
  "]+", flags=re.UNICODE)
  camada4 = Retirar_Emojis.sub('',camada3)
  camada5 = re.sub('#\S+','',camada4)
  camada6 = re.sub('[0-9]', '',camada5)


  return camada6

dados_modelo = base_dados[['Text','Classificacao']]
dados_modelo.shape

dados_modelo['Text'] = dados_modelo['Text'].apply(limpar_dados)

dados_modelo.head()

## STOP WORDS
grupo_palavras = nltk.corpus.stopwords.words('portuguese')
len(grupo_palavras)

def remover_stop_words(texto):
  lista = texto.split()
  frase_ajustada = ''
  for i in lista:
    if i not in grupo_palavras:
      frase_ajustada = frase_ajustada+ ' ' + i
  return frase_ajustada

dados_modelo['Text'] = dados_modelo['Text'].apply(remover_stop_words)

dados_modelo.head()

# EXTRAIR RADICAL
Radical = nltk.stem.RSLPStemmer()

def extrair_radical(texto):
  lista = texto.split()
  frases = ''
  for i in lista:
    extracao = Radical.stem(i)
    frases = frases+ ' ' + extracao
  return frases

dados_modelo['Text'] = dados_modelo['Text'].apply(extrair_radical)
dados_modelo.head()

nltk.download('punkt')

from nltk.tokenize import word_tokenize



"""MODELO

"""

from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import TweetTokenizer

frases_teste = dados_modelo['Text']
classes = dados_modelo['Classificacao']

from sklearn.model_selection import train_test_split

x_treino,x_teste,y_treino,y_teste = train_test_split(
    frases_teste,classes,test_size = 0.2,random_state = 10
)

len(x_treino)

token = TweetTokenizer()
vectorizar = CountVectorizer(analyzer='word',tokenizer=token.tokenize)

freq_palavras = vectorizar.fit_transform(x_treino)

from sklearn.naive_bayes import MultinomialNB
modelo_nb = MultinomialNB()

modelo_nb.fit(freq_palavras,y_treino)



"""**AVALIACAO**"""

vetorx_teste = vectorizar.transform(x_teste)

previsoes = modelo_nb.predict(vetorx_teste)

from sklearn.metrics import confusion_matrix
confusao = confusion_matrix(y_teste,previsoes)

from sklearn.metrics import classification_report
avaliacao =  classification_report(y_teste,previsoes)
avaliacao

def consolidar(texto):
  limpeza = limpar_dados(texto)
  stop = remover_stop_words(limpeza)
  rad = extrair_radical(stop)
  vet = vectorizar.transform([rad])
  pre = modelo_nb.predict(vet)
  return(pre)

consolidar("dengue")[0]

# outros modelos

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score


algoritimos = [
    LogisticRegression(),
    RandomForestClassifier(n_estimators=300,max_depth=12),
    MultinomialNB(),
    SVC()
]
lista_nome = ['regressao logistica','floresta aleatoria','naive bayes','vetor de maquinas']

resutado_cuacia = []
for nome,modelo in zip(lista_nome,algoritimos):
  algo = modelo
  algo.fit(freq_palavras,y_treino)
  vetor_teste = vectorizar.transform(x_teste)
  prev_teste = algo.predict(vetor_teste)
  resultado = accuracy_score(y_teste,prev_teste)
  resutado_cuacia.append(resultado)


dicionario = {
    'nome': lista_nome,
    'resutado': resutado_cuacia
}
   




